/*
*************************************************************************************************************************************
*************************************************************************************************************************************
*************************************************************************************************************************************
*Step 0: Create Data Lake	
*************************************************************************************************************************************
*************************************************************************************************************************************
*************************************************************************************************************************************
*/


--THIS IS DONE ON API UPLOAD SNA
-- read from the public s3 file. Then write to the folder structure.
-- SELECT * FROM readJSON(s3), write to paritioned BY year/month/date

--what is partition strategy of this lake??

/*
*************************************************************************************************************************************
*************************************************************************************************************************************
*************************************************************************************************************************************
*Step 1: Read from data lake	
*************************************************************************************************************************************
*************************************************************************************************************************************
*************************************************************************************************************************************
*/
CREATE OR REPLACE TABLE data_lake_shippingCostsByCountry AS
SELECT 
    *
    --mind the wild card here
FROM read_ndjson(
    '${s3BucketRoot}/tenants/tenantid=${tenantId}/tablename=defaultsnapshot_shippingCostsByCountry/*/*.jsonl.gz',
    hive_partitioning = true
);


--Read from existing compacted for this partition?



/*
*************************************************************************************************************************************
*************************************************************************************************************************************
********************* Step 2: Create Models	(no files here below)							***************************************
*************************************************************************************************************************************
*************************************************************************************************************************************
*/

CREATE OR REPLACE TABLE model_shippingCostsByCountry AS 


SELECT 

  list(json_object(
  
  'countryName', countryName, 
  'region', region, 
  'countryCode', countryCode,
  'shippingCost', shippingCost::double, 
  'returnShippingCost', returnShippingCost::double
  
  ))::JSON[] AS jsonRecord
FROM data_lake_shippingCostsByCountry;



/*
*************************************************************************************************************************************
*************************************************************************************************************************************
********************* Step 3: WRITE TO OUTPUT TABLES											***************************************
*************************************************************************************************************************************
*************************************************************************************************************************************
*/

-- Step 3: output

COPY model_shippingCostsByCountry TO '${s3BucketRoot}/tenants/tenantid=${tenantId}/transposed-output/tablename=lookups/model_shippingCostsByCountry.parquet'
(FORMAT parquet, COMPRESSION zstd);




