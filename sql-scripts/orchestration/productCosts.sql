/*
*************************************************************************************************************************************
*************************************************************************************************************************************
*************************************************************************************************************************************
*Step 0: Create Data Lake	
*************************************************************************************************************************************
*************************************************************************************************************************************
*************************************************************************************************************************************
*/


--THIS IS DONE ON API UPLOAD SNA
-- read from the public s3 file. Then write to the folder structure.
-- SELECT * FROM readJSON(s3), write to paritioned BY year/month/date

--what is partition strategy of this lake??

/*
*************************************************************************************************************************************
*************************************************************************************************************************************
*************************************************************************************************************************************
*Step 1: Read from data lake	
*************************************************************************************************************************************
*************************************************************************************************************************************
*************************************************************************************************************************************
*/
CREATE OR REPLACE TABLE data_lake_productCosts AS
SELECT 
    *
    --mind the wild card here
FROM read_ndjson(
    '${s3BucketRoot}/tenants/tenantid=${tenantId}/tablename=defaultsnapshot_productCosts/*/*.jsonl.gz',
    hive_partitioning = true
);


--Read from existing compacted for this partition?



/*
*************************************************************************************************************************************
*************************************************************************************************************************************
********************* Step 2: Create Models	(no files here below)							***************************************
*************************************************************************************************************************************
*************************************************************************************************************************************
*/

CREATE OR REPLACE TABLE model_productCosts AS 


SELECT 

variantId ::Varchar as variantId,
productId ::Varchar as productId,
productCost ::DOUBLE as productCost,
packingFee ::double as packingFee,
title ::varchar as title,
sku ::varchar as sku,
price ::double as price


FROM data_lake_productCosts;


/*
*************************************************************************************************************************************
*************************************************************************************************************************************
********************* Step 3: WRITE TO OUTPUT TABLES											***************************************
*************************************************************************************************************************************
*************************************************************************************************************************************
*/

-- Step 3: output

COPY model_productCosts TO '${s3BucketRoot}/tenants/tenantid=${tenantId}/transposed-output/tablename=lookups/model_productCosts.parquet'
(FORMAT parquet, COMPRESSION zstd);




