/*
*************************************************************************************************************************************
*************************************************************************************************************************************
*************************************************************************************************************************************
*Step 0: Create Data Lake	
*************************************************************************************************************************************
*************************************************************************************************************************************
*************************************************************************************************************************************
*/


--THIS IS DONE ON API UPLOAD SNA
-- read from the public s3 file. Then write to the folder structure.
-- SELECT * FROM readJSON(s3), write to paritioned BY year/month/date

--what is partition strategy of this lake??

/*
*************************************************************************************************************************************
*************************************************************************************************************************************
*************************************************************************************************************************************
*Step 1: Read from data lake	
*************************************************************************************************************************************
*************************************************************************************************************************************
*************************************************************************************************************************************
*/
CREATE OR REPLACE TABLE data_lake_costScheduler AS
SELECT 
    *
FROM read_ndjson(
    '${s3BucketRoot}/tenants/tenantid=${tenantId}/tablename=defaultsnapshot_costScheduler/*/*.jsonl.gz',
    hive_partitioning = true
);


--Read from existing compacted for this partition?



/*
*************************************************************************************************************************************
*************************************************************************************************************************************
********************* Step 2: Create Models	(no files here below)							***************************************
*************************************************************************************************************************************
*************************************************************************************************************************************
*/

-- create orders
CREATE OR REPLACE TABLE model_costScheduler AS
SELECT 
    Id :: VARCHAR AS Id,
	ExpenseName :: VARCHAR AS ExpenseName,
	Status :: VARCHAR AS Status,
	Category :: VARCHAR AS Category,
	Cost :: DOUBLE AS Cost,
	Frequency :: VARCHAR AS Frequency,
	Tags :: VARCHAR AS Tags,
	StartDate :: DATE AS StartDate,
	COALESCE(NULLIF(EndDate, ''), '9999-01-01'):: DATE AS EndDate 
  
  
FROM data_lake_costScheduler;


/*
*************************************************************************************************************************************
*************************************************************************************************************************************
********************* Step 3: WRITE TO OUTPUT TABLES											***************************************
*************************************************************************************************************************************
*************************************************************************************************************************************
*/

-- Step 3: output

COPY model_costScheduler TO '${s3BucketRoot}/tenants/tenantid=${tenantId}/transposed-output/tablename=model_costScheduler/data.parquet'
(FORMAT parquet, COMPRESSION zstd);


