/*
*************************************************************************************************************************************
*************************************************************************************************************************************
*************************************************************************************************************************************
*Step 0: Create Data Lake	
*************************************************************************************************************************************
*************************************************************************************************************************************
*************************************************************************************************************************************
*/


--THIS IS DONE ON API UPLOAD SNA
-- read from the public s3 file. Then write to the folder structure.
-- SELECT * FROM readJSON(s3), write to paritioned BY year/month/date

--what is partition strategy of this lake??

/*
*************************************************************************************************************************************
*************************************************************************************************************************************
*************************************************************************************************************************************
*Step 1: Read from data lake	
*************************************************************************************************************************************
*************************************************************************************************************************************
*************************************************************************************************************************************
*/
CREATE OR REPLACE TABLE data_lake_orderIdsWithShippingCost AS
SELECT 
    *
    --mind the wild card here
FROM read_ndjson(
    '${s3BucketRoot}/tenants/tenantid=${tenantId}/tablename=shippingCosts/*/*.jsonl.gz',
    hive_partitioning = true
);


--Read from existing compacted for this partition?



/*
*************************************************************************************************************************************
*************************************************************************************************************************************
********************* Step 2: Create Models	(no files here below)							***************************************
*************************************************************************************************************************************
*************************************************************************************************************************************
*/

CREATE OR REPLACE TABLE model_orderIdsWithShippingCost AS 


SELECT 
	orderId ::varchar as orderId,
	shippingCost :: double as shippingCost

FROM data_lake_orderIdsWithShippingCost;


/*
*************************************************************************************************************************************
*************************************************************************************************************************************
********************* Step 3: WRITE TO OUTPUT TABLES										***************************************
*************************************************************************************************************************************
*************************************************************************************************************************************
*/

-- Step 3: output

COPY model_orderIdsWithShippingCost TO '${s3BucketRoot}/tenants/tenantid=${tenantId}/transposed-output/tablename=lookups/model_orderIdsWithShippingCost.parquet'
(FORMAT parquet, COMPRESSION zstd);
SELECT * FROM model_orderIdsWithShippingCost;




